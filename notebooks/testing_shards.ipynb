{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6421cc6d-2f9b-4d51-b2c3-e7184a870b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, numpy as np, sentencepiece as spm, random, json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665a306a-3c00-4578-a0be-f1086380c771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shards = 2, val shards = 1\n",
      "shape (10000, 512) dtype uint16\n",
      "saw a small bird named Tim. Max said, \"Tim, can you help me open this sack?\" Tim said, \"Yes, I can help you.\" Tim could whistle very loud. Tim whistled loud to call his bird friends. Many birds came to help Max. They all pulled the sack open. Inside the sack, they found lots of yummy treats. Max and the birds were very happy. They shared the treats and had a fun day at the park.<|endoftext|> One day, a boy named\n"
     ]
    }
   ],
   "source": [
    "import pathlib, glob, numpy as np, sentencepiece as spm, random\n",
    "\n",
    "proj_root   = pathlib.Path.cwd().parent   \n",
    "tokens_root = proj_root / \"storage\" / \"tokens\"\n",
    "\n",
    "train_shards = glob.glob(str(tokens_root / \"train\" / \"tinystories_nano\" / \"tokens_*.npz\"))\n",
    "val_shards   = glob.glob(str(tokens_root / \"val\"   / \"tinystories_nano\" / \"tokens_*.npz\"))\n",
    "\n",
    "print(f\"train shards = {len(train_shards)}, val shards = {len(val_shards)}\")\n",
    "arr = np.load(random.choice(train_shards))[\"input_ids\"]\n",
    "print(\"shape\", arr.shape, \"dtype\", arr.dtype)\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=str(tokens_root / \"tokenizer\" / \"tokenizer.model\"))\n",
    "print(sp.decode(arr[0][:100].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a71bd2f1-fbaa-4f41-b9e3-b388980c2818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, glob, itertools\n",
    "\n",
    "class NPZChunks(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, pattern, shuffle=True):\n",
    "        self.files   = glob.glob(pattern, recursive=True)\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        files = self.files.copy()\n",
    "        if self.shuffle: random.shuffle(files)\n",
    "        for fname in files:\n",
    "            data = np.load(fname)[\"input_ids\"]          # (N, 512) uint16\n",
    "            idxs = range(len(data))\n",
    "            if self.shuffle: idxs = random.sample(idxs, len(idxs))\n",
    "            for i in idxs:\n",
    "                yield torch.from_numpy(data[i]).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a8bc2f0-82c0-49aa-917a-30f59da86273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "proj_root = pathlib.Path.cwd().parent   \n",
    "pattern = str(proj_root / \"storage/tokens/train/tinystories_nano/*.npz\")\n",
    "train_dl = DataLoader(\n",
    "    NPZChunks(pattern),\n",
    "    batch_size=8, num_workers=1, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24271643-fc9c-4d97-bd0c-8761e48beb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  184,    13,    78,  ..., 29975,   104,    92],\n",
       "        [  402, 29982, 29955,  ...,   272,    31,   137],\n",
       "        [   96,   519,   225,  ...,   172,    86,    31],\n",
       "        ...,\n",
       "        [  117,    11,   438,  ...,  1846,    66,    11],\n",
       "        [  224,    86,  4072,  ...,  1502,   114,   205],\n",
       "        [ 1263,    13,  2181,  ..., 29967,    59,  5255]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49da0958-5f9f-4e37-89e0-9a9d62e1a947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader: 7,649,913 tokens/s\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "warmup_batches = 10\n",
    "measure_batches = 100\n",
    "tok_per_batch = 8 * 512                 # batch_size * seq_len\n",
    "\n",
    "t0 = time.time()\n",
    "for i, batch in enumerate(train_dl):\n",
    "    if i == warmup_batches:\n",
    "        t0 = time.time()                # start timing after warm-up\n",
    "    if i == warmup_batches + measure_batches:\n",
    "        break\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"DataLoader: {measure_batches * tok_per_batch / elapsed:,.0f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b33e21-2808-4514-85d9-a4c0470a969b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
