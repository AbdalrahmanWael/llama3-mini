[project]
name = "llama3-mini"
version = "0.1.0"
description = "Minimal Llama-3 implementation with MoE support"
authors = [
    {name = "Golden", email = "abdalrahman@okiynai.com"},
]
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "torch>=2.1.0",
    "torchvision",
    "torchaudio",
    "numpy",
    "tqdm",
    "pyyaml",
    "datasets",
    "transformers",
    "tokenizers",
    "sentencepiece",
	"einops==0.8.0",
    "matplotlib",
    "seaborn",
    "wandb",
    "accelerate",
    "safetensors",
]

[project.optional-dependencies]
gpu = [
    "flash-attn>=2.7.4; platform_machine=='x86_64'",
]
dev = [
    "pytest>=7.0.0",
    "black>=23.0.0",
    "isort>=5.0.0",
    "flake8>=6.0.0",
    "jupyter",
    "ipykernel",
]
moe = [
    "triton>=2.1.0",
]

no-build-isolation-package = ["flash-attn"]

[[tool.uv.dependency-metadata]]
name = "flash-attn"
version = "2.7.4post1"
requires-dist = ["torch", "einops"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/llama3_mini"]

[tool.uv]
dev-dependencies = [
    "pytest>=7.0.0",
    "black>=23.0.0",
    "isort>=5.0.0",
    "flake8>=6.0.0",
    "jupyter>=1.0.0",
    "ipykernel>=6.0.0",
]

[tool.black]
line-length = 88
target-version = ['py310']

[tool.isort]
profile = "black"
multi_line_output = 3
